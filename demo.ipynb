{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import our custom modules\n",
    "from src.ingestion import IngestionEngine, TextChunker\n",
    "from src.embedder import Embedder\n",
    "from src.vector_store import VectorStore\n",
    "from src.answer_engine import AnswerEngine\n",
    "\n",
    "# Load Env Vars (API Keys)\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded successfully.\n",
      "‚úÖ Connected to Local Vector Store (ChromaDB) at ./chroma_db\n",
      "Loading local embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded successfully.\n",
      "‚úÖ Connected to Local Vector Store (ChromaDB) at ./chroma_db\n",
      "‚úÖ Pipeline components initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline components\n",
    "ingestor = IngestionEngine()\n",
    "chunker = TextChunker()\n",
    "embedder = Embedder()\n",
    "vector_db = VectorStore()\n",
    "engine = AnswerEngine()\n",
    "\n",
    "print(\"‚úÖ Pipeline components initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 1 document(s).\n",
      "üß© Split into 1 chunks.\n",
      "üß† Generated 1 embeddings.\n",
      "Successfully stored 1 chunks locally.\n",
      "üíæ Saved to Vector Store.\n"
     ]
    }
   ],
   "source": [
    "# Define a test file (Create a dummy file if needed)\n",
    "test_file = \"test_document.txt\"\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(\"The project architecture uses a decoupled design. It relies on ChromaDB for vector storage and Groq for LLM inference.\")\n",
    "\n",
    "# 1. Load\n",
    "docs = ingestor.load_file(test_file)\n",
    "print(f\"üìÑ Loaded {len(docs)} document(s).\")\n",
    "\n",
    "# 2. Chunk\n",
    "chunks = chunker.chunk_documents(docs)\n",
    "print(f\"üß© Split into {len(chunks)} chunks.\")\n",
    "\n",
    "# 3. Embed\n",
    "texts = [d.content for d in chunks]\n",
    "vectors = embedder.embed(texts)\n",
    "print(f\"üß† Generated {len(vectors)} embeddings.\")\n",
    "\n",
    "# 4. Store\n",
    "# Since our VectorStore is async, we use asyncio to run it in a notebook\n",
    "await vector_db.add_documents(chunks, vectors)\n",
    "print(\"üíæ Saved to Vector Store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What database does this project use?\n",
      "\n",
      "Analyzing query: What database does this project use?...\n",
      "ü§ñ Answer below :\n",
      "The project uses ChromaDB for vector storage.\n"
     ]
    }
   ],
   "source": [
    "query = \"What database does this project use?\"\n",
    "\n",
    "print(f\"‚ùì Question: {query}\\n\")\n",
    "\n",
    "# Run the Answer Engine\n",
    "response = await engine.answer(query)\n",
    "\n",
    "print(\"ü§ñ Answer below :\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
